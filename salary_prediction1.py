# -*- coding: utf-8 -*-
"""Salary_Prediction1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IDhQ32qHuepxg6Mi_0rdTJLFMlzgSIxe
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error,mean_absolute_error

#loding the dataset
df = pd.read_csv(r'/content/drive/MyDrive/Salary_Data.csv')
df

df.info()

df.describe()

df.isnull().sum()

df.dropna(inplace=True)

df['Job Title'].value_counts()

job_title_count = df['Job Title'].value_counts()
job_title_edited = job_title_count[job_title_count<=25]
job_title_edited.count()

df['Job Title'] = df['Job Title'].apply(lambda x: 'Others' if x in job_title_edited else x )
df['Job Title'].nunique()

df['Education Level'].value_counts()

df['Education Level'].replace(["Bachelor's Degree","Master's Degree","phD"],["Bachelor's","Master's","PhD"],inplace=True)
df['Education Level'].value_counts()

df['Gender'].value_counts()

#detecting the outliers in the salary column using iqr method

Q1=df.Salary.quantile(0.25)
Q3=df.Salary.quantile(0.75)

IQR=Q3-Q1

lower=Q1-1.5*IQR
upper=Q3+1.5*IQR

df[df.Salary>upper]

df[df.Salary< lower]

# Create a ranking of Job Titles based on median Salary
title_salary = df.groupby('Job Title')['Salary'].median().sort_values()
title_rank = {title: rank for rank, title in enumerate(title_salary.index, start=1)}

# Map the ranks to the DataFrame
df['Job_Title_Rank'] = df['Job Title'].map(title_rank)

# Drop original Job Title column
df.drop('Job Title', axis=1, inplace=True)

df.head(15)

df['Education Level'].replace({
    "Bachelor's Degree": "Bachelor's",
    "Master's Degree": "Master's",
    "phD": "PhD"
}, inplace=True)


df = pd.get_dummies(df, columns=['Gender', 'Education Level'], drop_first=True)

from sklearn.model_selection import train_test_split

# Separate features and target
X = df.drop('Salary', axis=1)
y = df['Salary']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.2f}")
print(f"Mean Absolute Error: {mae:.2f}")

import matplotlib.pyplot as plt

# Feature importance from model
importances = model.feature_importances_
features = X.columns
indices = importances.argsort()[::-1]

# Plot
plt.figure(figsize=(10,6))
plt.title("Feature Importance")
plt.bar(range(len(features)), importances[indices], align='center')
plt.xticks(range(len(features)), features[indices], rotation=90)
plt.tight_layout()
plt.show()

from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

grid = GridSearchCV(RandomForestRegressor(random_state=42), param_grid=params, cv=5, scoring='neg_mean_squared_error')
grid.fit(X_train, y_train)

print("Best Parameters:", grid.best_params_)

from sklearn.metrics import r2_score

# Use best model from GridSearchCV
best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Final Model Evaluation:")
print(f"  RÂ² Score: {r2:.4f}")
print(f"  MAE: {mae:.2f}")
print(f"  MSE: {mse:.2f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(y_test, y_pred, alpha=0.6)
plt.xlabel("Actual Salary")
plt.ylabel("Predicted Salary")
plt.title("Actual vs Predicted Salary")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # 45-degree line
plt.grid(True)
plt.tight_layout()
plt.show()

import joblib

joblib.dump(best_model, "final_salary_model.pkl")

model = joblib.load("final_salary_model.pkl")

#Step 1: Get expected columns from training
expected_features = X.columns  # X is your training feature DataFrame

# Step 2: Create a new employee data item
new_data = pd.DataFrame({
    'Age': [30],
    'Years of Experience': [23],
    'Job_Title_Rank': [title_rank['Software Engineer']],
    'Gender_Male': [1],
    'Gender_Other': [0],  # add missing
    'Education Level_Master\'s': [1],
    'Education Level_PhD': [0],
    'Education Level_High School': [0]  # add if it was in original
})

# Step 3: Ensure all missing columns are filled with 0
for col in expected_features:
    if col not in new_data.columns:
        new_data[col] = 0

# Step 4: Ensure correct column order
new_data = new_data[expected_features]

# Step 5: Predict
predicted_salary = best_model.predict(new_data)
print(f"Predicted Salary: {predicted_salary[0]:,.2f}")

#2nd employee data item
new_data2 = pd.DataFrame({
    'Age': [42],
    'Years of Experience': [18],
    'Job_Title_Rank': [title_rank['Software Engineer']],  # Change title as needed
    'Gender_Male': [0],
    'Gender_Other': [0],
    'Education Level_Master\'s': [0],
    'Education Level_PhD': [1],
    'Education Level_High School': [0]
})

# Step 3: Ensure all missing columns are filled with 0
for col in expected_features:
    if col not in new_data2.columns:
        new_data2[col] = 0

# Step 4: Ensure correct column order
new_data2 = new_data2[expected_features]

# Step 5: Predict
predicted_salary = best_model.predict(new_data2)
print(f"Predicted Salary: {predicted_salary[0]:,.2f}")